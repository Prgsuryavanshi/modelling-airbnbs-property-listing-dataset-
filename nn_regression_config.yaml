# optimiser : sgd, adam & adagrad
optimiser: ['sgd', 'adam', 'adagrad']
input_node_size: 11
output_node_size: 1
learning_rate: [0.001, 0.01, 0.1]
hidden_layer_width: [32, 64]
# model_depth includes no of hidden layers + output layer
model_depth: [2, 3, 4, 5, 6]
epoch: 200
