{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfcd627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"C:/Users/Anurag/Documents/python/neural network/neural network\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28cd6491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import abc\n",
    "import json\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "\n",
    "from tabular_data import load_airbnb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a998ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3f6de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train & test data using train_test split()\n",
    "def get_split_data(features: pd.core.frame.DataFrame, label: pd.core.series.Series,task_folder: str) -> tuple[pd.core.frame.DataFrame, pd.core.series.Series, pd.core.frame.DataFrame, pd.core.series.Series, pd.core.frame.DataFrame, pd.core.series.Series]:\n",
    "    \"\"\"This function is used to:\n",
    "       -> get numeric data from dataset and Price_Night/Category as label based on task folder.\n",
    "       -> normalize the dataset and split it into the train, test and validation dataset.\n",
    "\n",
    "    Args:\n",
    "        features (pd.core.frame.DataFrame): training dataset\n",
    "        label (pd.core.series.Series): target data\n",
    "        task_folder (str): regression/classification task\n",
    "\n",
    "    Returns:\n",
    "        x_train (pd.core.frame.DataFrame): independent training dataset.\n",
    "        y_train (pd.core.frame.DataFrame): dependent training dataset.\n",
    "        x_val (pd.core.frame.DataFrame): independent validation dataset.\n",
    "        y_val (pd.core.frame.DataFrame): dependent validation dataset.\n",
    "        x_test (pd.core.frame.DataFrame): independent testing dataset.\n",
    "        y_test (pd.core.frame.DataFrame): dependent testing dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    if task_folder=='models/classification':\n",
    "        # split the scaled_dataset into train+val and test data\n",
    "        # due to class imbalance, stratify option is used in train_test_split() to have equal \n",
    "        # distribution of all output classes\n",
    "        x_trainval, x_test, y_trainval, y_test = train_test_split(features, label, stratify=label, test_size=0.10)\n",
    "\n",
    "        # split the trainval into train and val data\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_trainval, y_trainval, stratify=y_trainval, test_size=0.15) \n",
    "    else:\n",
    "        # split the scaled_dataset into train+val and test data\n",
    "        x_trainval, x_test, y_trainval, y_test = train_test_split(features, label, test_size=0.10)\n",
    "\n",
    "        # split the trainval into train and val data\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_trainval, y_trainval, test_size=0.15)\n",
    "        \n",
    "    # normalize x_train, x_test & x_val\n",
    "    scaler = MinMaxScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    x_val = scaler.transform(x_val)\n",
    "    # numpy -> dataframe\n",
    "    x_train = pd.DataFrame(x_train, columns = features.columns)\n",
    "    x_val = pd.DataFrame(x_val, columns = features.columns)\n",
    "    x_test = pd.DataFrame(x_test, columns = features.columns)\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2cd9516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tune_regression_model_hyperparameters(model_class: abc.ABCMeta, split_data: tuple, hyper_params: dict) -> tuple[str, dict, dict]:\n",
    "    \"\"\"This function is used to: \n",
    "        -> perform custom grid search by hyperparameters tuning.\n",
    "        -> calculate R2, MSE, RMSE and MAE of train, test and val data, and \n",
    "        -> find best model based on validation rmse score.\n",
    "\n",
    "    Args:\n",
    "        model_class (abc.ABCMeta): model class.\n",
    "        split_data (tuple): training, validation, and test sets.\n",
    "        hyper_params (dict): dictionary of hyperparameters values to get best model based on 'validation_rmse'.\n",
    "    \n",
    "    Returns:\n",
    "        best_model (str): return the model with best score.\n",
    "        best_hyperparameter_values (dict): return a dictionary of its best hyperparameter values.\n",
    "        model_score (dict): return a dictionary of its best performance metrics.\n",
    "    \"\"\"\n",
    "    # unpack split_data\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = split_data\n",
    "    # variable to store best val rmse for comparison\n",
    "    best_val_rmse = float('inf')\n",
    "    # empty list for performance metrics\n",
    "    model_score = {\"test_r2\":[], \"train_r2\":[], \"val_r2\":[],\"test_RMSE\":[], \"train_RMSE\":[], \"validation_RMSE\":[], \"test_MSE\":[], \"train_MSE\":[],  \"val_MSE\":[], \"test_MAE\":[], \"train_MAE\":[], \"val_MAE\":[], \"Model_Class\":[]}\n",
    "    # unpack key value pairs from hyper_params dict\n",
    "    keys, values = zip(*hyper_params.items())\n",
    "    permutations_hyper_params = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    # train model with hyper params\n",
    "    for params in permutations_hyper_params:\n",
    "        model = model_class(**params)\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        # predict target using x_test data\n",
    "        ypred_test = model.predict(x_test)\n",
    "\n",
    "        # predict target using x_train data\n",
    "        ypred_train = model.predict(x_train)\n",
    "        \n",
    "        # predict target using x_val data\n",
    "        ypred_val = model.predict(x_val)\n",
    "        # get performance metrics based on best val_rmse\n",
    "        val_rmse = mean_squared_error(y_val,ypred_val,squared=False)\n",
    "        if val_rmse < best_val_rmse:\n",
    "            # get best model\n",
    "            best_model = model\n",
    "            # if new val_rmse is less than best_val_rmse \n",
    "            best_val_rmse = val_rmse\n",
    "            # get model class name\n",
    "            model_score[\"Model_Class\"] = str(model.__class__.__name__)\n",
    "            # get best hyper param\n",
    "            best_hyperparameter_values = params\n",
    "            # get model score\n",
    "            model_score[\"test_r2\"] = r2_score(y_test, ypred_test)\n",
    "            model_score[\"train_r2\"] = r2_score(y_train, ypred_train)\n",
    "            model_score[\"val_r2\"] = r2_score(y_val, ypred_val)\n",
    "            model_score[\"train_MAE\"] = mean_absolute_error(y_train,ypred_train)\n",
    "            model_score[\"test_MAE\"] = mean_absolute_error(y_test,ypred_test)\n",
    "            model_score[\"val_MAE\"] = mean_absolute_error(y_val,ypred_val)\n",
    "            model_score[\"train_MSE\"]= mean_squared_error(y_train,ypred_train)\n",
    "            model_score[\"test_MSE\"]= mean_squared_error(y_test,ypred_test)\n",
    "            model_score[\"val_MSE\"]= mean_squared_error(y_val,ypred_val)\n",
    "            model_score[\"validation_RMSE\"] = val_rmse\n",
    "            model_score[\"train_RMSE\"] = mean_squared_error(y_train,ypred_train,squared=False)\n",
    "            model_score[\"test_RMSE\"] = mean_squared_error(y_test,ypred_test,squared=False)\n",
    "    \n",
    "    return best_model, best_hyperparameter_values, model_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e45614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milestone-4, Task-4 & 6\n",
    "def tune_regression_model_hyperparameters(model_class: abc.ABCMeta, hyper_params: dict, split_data: tuple, validation: int=5) -> tuple[abc.ABCMeta, dict, dict]:\n",
    "    \"\"\"This function is used to:\n",
    "        -> to perform a grid search using SKLearn's GridSearchCV over a reasonable range of hyperparameter values,\n",
    "        -> train regression model using best hyper params obtained from GridSearchCV() and\n",
    "        -> get best model performance metrics\n",
    "        \n",
    "    Args:\n",
    "        model_class (abc.ABCMeta): the model class to be optimized by cross-validated grid-search over a parameter grid.\n",
    "        hyper_params (dict): a dictionary of hyperparameter names mapping to a list of values to be tried.\n",
    "        split_data (tuple): training, validation, and test sets.\n",
    "        validation (int, optional): the cross-validation splitting strategy. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        trained_model (abc.ABCMeta): the model class trained and tuned using GridSearchCV().\n",
    "        best_model_param (dict): a dictionary of best hyperparameters on which the model is trained.\n",
    "        model_score (dict): a dictionary of performance metrics of the model once it is trained and tuned.\n",
    "    \"\"\"\n",
    "    # unpack split_data\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = split_data \n",
    "    \n",
    "    #find best parameters using GridSearchCV()\n",
    "    grid = GridSearchCV(estimator=model_class, param_grid=hyper_params, cv=validation, scoring='neg_root_mean_squared_error', verbose=1, n_jobs=-1)  \n",
    "    grid_result = grid.fit(x_train, y_train)\n",
    "    best_model_param = grid_result.best_params_\n",
    "    \n",
    "    # get model class and create an instance of model_class\n",
    "    reg_model = model_class.__class__\n",
    "    reg_model = reg_model(**best_model_param)\n",
    "    # train model\n",
    "    trained_model = reg_model.fit(x_train, y_train)\n",
    "    # store score in model_score\n",
    "    model_score = regression_trained_model_metrics(trained_model, split_data)\n",
    "\n",
    "    return trained_model, model_score, best_model_param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbe045da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_trained_model_metrics(trained_model: abc.ABCMeta, split_data: tuple) -> dict:\n",
    "    \"\"\"This function caculates the performance metrics of the trained model(trained and tuned using GridSearchCV) \n",
    "       and append it to the model_score dictionary.\n",
    "\n",
    "    Args:\n",
    "        trained_model (abc.ABCMeta): the model class trained and tuned using GridSearchCV.\n",
    "        split_data (tuple): training, validation, and test sets.\n",
    "\n",
    "    Returns:\n",
    "        model_score (dict): a dictionary of performance metrics.\n",
    "    \"\"\"\n",
    "    # unpack split_data\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = split_data\n",
    "    # empty list to store regression model performance metrics\n",
    "    model_score = {\"test_r2\":[],\n",
    "                   \"train_r2\":[],\n",
    "                   \"val_r2\":[],\n",
    "                   \"test_RMSE\":[],\n",
    "                   \"train_RMSE\":[],\n",
    "                   \"validation_RMSE\":[],\n",
    "                   \"test_MSE\":[],\n",
    "                   \"train_MSE\":[],\n",
    "                   \"val_MSE\":[],\n",
    "                   \"test_MAE\":[],\n",
    "                   \"train_MAE\":[],\n",
    "                   \"val_MAE\":[],\n",
    "                   \"Model_Class\":[]}\n",
    "    \n",
    "    # predict target using x_test data\n",
    "    ypred_test = trained_model.predict(x_test)\n",
    "\n",
    "    # predict target using x_train data\n",
    "    ypred_train = trained_model.predict(x_train)\n",
    "\n",
    "    # predict target using x_val data\n",
    "    ypred_val = trained_model.predict(x_val)\n",
    "    \n",
    "    # save model_class name\n",
    "    model_score[\"Model_Class\"].append(str(trained_model.__class__.__name__))\n",
    "    \n",
    "    # append performance metrics to model_score\n",
    "    model_score[\"test_r2\"].append(r2_score(y_test, ypred_test))\n",
    "    model_score[\"train_r2\"].append(r2_score(y_train, ypred_train))\n",
    "    model_score[\"val_r2\"].append(r2_score(y_val, ypred_val))\n",
    "    model_score[\"train_MAE\"].append(mean_absolute_error(y_train,ypred_train))\n",
    "    model_score[\"test_MAE\"].append(mean_absolute_error(y_test,ypred_test))\n",
    "    model_score[\"val_MAE\"].append(mean_absolute_error(y_val,ypred_val))\n",
    "    model_score[\"train_MSE\"].append(mean_squared_error(y_train,ypred_train))\n",
    "    model_score[\"test_MSE\"].append(mean_squared_error(y_test,ypred_test))\n",
    "    model_score[\"val_MSE\"].append(mean_squared_error(y_val,ypred_val))\n",
    "    model_score[\"validation_RMSE\"].append(mean_squared_error(y_val,ypred_val,squared=False))\n",
    "    model_score[\"train_RMSE\"].append(mean_squared_error(y_train,ypred_train,squared=False))\n",
    "    model_score[\"test_RMSE\"].append(mean_squared_error(y_test,ypred_test,squared=False))\n",
    "    \n",
    "    return model_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ce3ad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milestone-4, Task-5\n",
    "def save_model(trained_model: abc.ABCMeta, best_model_param: dict, model_score: dict, folder: str):\n",
    "    \"\"\"This function saves:\n",
    "       -> the model in a file called model.joblib (SKLearn regression & classification model) and model.pt(PyTorch)\n",
    "       -> its hyperparameters in a file called hyperparameters.json.\n",
    "       -> its performance metrics in a file called metrics.json once it's trained and tuned.\n",
    "       The function take in the name of a folder where these files are saved as a keyword argument \"folder\".\n",
    "\n",
    "    Args:\n",
    "        trained_model (abc.ABCMeta): the model class trained and tuned using GridSearchCV.\n",
    "        best_model_param (dict): a dictinary of best hyperparameters on which the model is trained.\n",
    "        model_score (dict): a dictinary of performance metrics of the model once it is trained and tuned.\n",
    "        folder (str): the name of a folder where these files are saved.\n",
    "    \"\"\"\n",
    "    # save pytorch model\n",
    "    if isinstance(trained_model, torch.nn.Module):\n",
    "        with open(os.path.join(folder, \"model.pt\"), \"wb\") as out_pt:\n",
    "            torch.save(trained_model.state_dict(), out_pt )\n",
    "    \n",
    "    # save regression/classification model\n",
    "    else:\n",
    "        with open(os.path.join(folder, \"model.joblib\"), \"wb\") as out_joblib:\n",
    "            joblib.dump(trained_model, out_joblib)\n",
    "    \n",
    "    # save model hyperparameters\n",
    "    with open(os.path.join(folder, \"hyperparameters.json\"), \"w\") as param_json:\n",
    "        json.dump(best_model_param, param_json, indent=4)\n",
    "    \n",
    "    # save model metrics\n",
    "    with open(os.path.join(folder, \"metrics.json\"), \"w\") as metrics_json:\n",
    "        json.dump(model_score, metrics_json, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b16cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milestone-4, Task-6\n",
    "# Milestone-5, Task-5\n",
    "def classification_trained_model_metrics(trained_model: abc.ABCMeta, split_data: tuple) -> dict:\n",
    "    \"\"\"This function caculates the performance metrics of the trained classifier model\n",
    "    (trained and tuned using GridSearchCV) and append it to the model_score dictionary.\n",
    "\n",
    "    Args:\n",
    "        trained_model (abc.ABCMeta): the model class trained and tuned using GridSearchCV.\n",
    "        split_data (tuple): training, validation, and test sets.\n",
    "\n",
    "    Returns:\n",
    "        model_score (dict): a dictionary of performance metrics.\n",
    "    \"\"\"\n",
    "    # unpack split_data\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = split_data\n",
    "    \n",
    "    # empty list to store classification model performance metrics\n",
    "    model_score = {\"accuracy\":{}, \"f1_score\":{}, \"precision\":{}, \"recall\":{}}\n",
    "\n",
    "    # predict target using x_test data\n",
    "    ypred_test = trained_model.predict(x_test)\n",
    "\n",
    "    # predict target using x_train data\n",
    "    ypred_train = trained_model.predict(x_train)\n",
    "\n",
    "    # predict target using x_val data\n",
    "    ypred_val = trained_model.predict(x_val)\n",
    "    \n",
    "    # calculate f1 score \n",
    "    test_f1 = f1_score(y_test, ypred_test, average='weighted')\n",
    "    train_f1 = f1_score(y_train, ypred_train, average='weighted')\n",
    "    val_f1 = f1_score(y_val, ypred_val, average='weighted')\n",
    "    \n",
    "    # calculate precision score \n",
    "    test_precision = precision_score(y_test, ypred_test, average='weighted')\n",
    "    train_precision = precision_score(y_train, ypred_train, average='weighted')\n",
    "    val_precision = precision_score(y_val, ypred_val, average='weighted')\n",
    "    \n",
    "    # calculate recall score \n",
    "    test_recall = recall_score(y_test, ypred_test, average='weighted')\n",
    "    train_recall = recall_score(y_train, ypred_train, average='weighted')\n",
    "    val_recall = recall_score(y_val, ypred_val, average='weighted')\n",
    "    \n",
    "    # calculate accuracy    \n",
    "    test_accuracy = accuracy_score(y_test, ypred_test, normalize=True)\n",
    "    train_accuracy = accuracy_score(y_train, ypred_train, normalize=True)\n",
    "    val_accuracy = accuracy_score(y_val, ypred_val, normalize=True)    \n",
    "    \n",
    "    # append scores/metrics to model_score\n",
    "    model_score['accuracy'] = {'train': train_accuracy, 'test': test_accuracy, 'val': val_accuracy}\n",
    "    model_score['precision'] = {'train': train_precision, 'test': test_precision, 'val': val_precision}\n",
    "    model_score['recall'] = {'train': train_recall, 'test': test_recall, 'val': val_recall}\n",
    "    model_score['f1_score']= {'train': train_f1, 'test': test_f1, 'val': val_f1}\n",
    "    \n",
    "    # save model_class name\n",
    "    model_score[\"Model_Class\"] = str(trained_model.__class__.__name__)\n",
    "\n",
    "    return model_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebaab011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_classification_model_hyperparameters(model_class: abc.ABCMeta, hyper_params: dict, split_data: tuple, validation: int=5) -> tuple[abc.ABCMeta, dict, dict]:\n",
    "    \"\"\"This function is used to:\n",
    "        -> to perform a grid search using SKLearn's GridSearchCV over a reasonable range of hyperparameter values,\n",
    "        -> train classification model using best hyper params obtained from GridSearchCV() and\n",
    "        -> get best model performance metrics\n",
    "\n",
    "    Args:\n",
    "        model_class (abc.ABCMeta): the model class to be optimized by cross-validated grid-search over a parameter grid.\n",
    "        hyper_params (dict): a dictionary of hyperparameter names mapping to a list of values to be tried.\n",
    "        split_data (tuple): training, validation, and test sets.\n",
    "        validation (int, optional): Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        trained_model (abc.ABCMeta): the model class trained and tuned using GridSearchCV.\n",
    "        best_model_param (dict): a dictionary of best hyperparameters on which the model is trained.\n",
    "        model_score (dict): a dictionary of performance metrics of the model once it is trained and tuned.\n",
    "    \"\"\"\n",
    "    # unpack split_data\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = split_data\n",
    "    \n",
    "    #find best parameters using GridSearchCV()\n",
    "    grid = GridSearchCV(estimator=model_class, param_grid=hyper_params, cv=validation, scoring='accuracy', verbose=1, n_jobs=-1)  \n",
    "    grid_result = grid.fit(x_train, y_train)\n",
    "    best_model_param = grid_result.best_params_\n",
    "    \n",
    "    # get model class and create an instance of model_class\n",
    "    clf_model = model_class.__class__\n",
    "    clf_model = clf_model(**best_model_param)\n",
    "    \n",
    "    # train model\n",
    "    trained_model = clf_model.fit(x_train, y_train)\n",
    "    \n",
    "    # store score in model_score\n",
    "    model_score = classification_trained_model_metrics(trained_model, split_data)\n",
    "\n",
    "    return trained_model, model_score, best_model_param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "097adecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Milestone-4, Task-6\n",
    "def evaluate_all_models(task_folder: str, split_data: tuple) -> None:\n",
    "    \"\"\"This function evaluate all the performance of the model(regressor and/or classifier) \n",
    "       by using different models provided by sklearn:\n",
    "       -> SGDRegressor(Linear regressor)\n",
    "       -> LogisticRegression(Linear classifier)\n",
    "       -> Decision trees (regressor and classifier)\n",
    "       -> Gradient boosting (regressor and classifier)\n",
    "       -> Random forests (regressor and classifier)\n",
    "       Save the model, hyperparameters, and metrics in a folder named after the model class.\n",
    "\n",
    "    Args:\n",
    "        task_folder (str): the name of the parent folder where evaluated regressor and classifier models are saved.\n",
    "        split_data (tuple): training, validation, and test sets.\n",
    "    \"\"\"\n",
    "    # model_class = SGDRegressor()\n",
    "    sgd_reg_parameters = {\n",
    "        'loss'         : ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "        'penalty'      : ['l1', 'l2', 'elasticnet', None],\n",
    "        'alpha'        : [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "        'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "        'epsilon'      : [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "        'eta0'         : [1, 10, 100],\n",
    "        'max_iter'     : [0, 0.5, 1, 10, 100, 1000, 5000, 10000]\n",
    "    }\n",
    "    # model_class = DecisionTreeRegressor()\n",
    "    decisiontree_reg_parameters = {\n",
    "        'criterion'               : ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "        'splitter'                : ['best', 'random'],\n",
    "        'max_depth'               : [1,3,5,7,9,11,12, None],\n",
    "        'min_samples_leaf'        : [1,2,3,4,5,6,7,8,9,10],\n",
    "        'min_weight_fraction_leaf': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "        'max_features'            : [\"log2\",\"sqrt\",None],\n",
    "        'max_leaf_nodes'          : [None,10,20,30,40,50,60,70,80,90]\n",
    "    }\n",
    "    # model_class = GradientBoostingRegressor()\n",
    "    gradientboosting_reg_parameters = {\n",
    "        'learning_rate': [0.01,0.04],\n",
    "        'n_estimators' : [100, 1500],\n",
    "        'subsample'    : [0.9, 0.1],\n",
    "        'max_depth'    : [4,10, None]\n",
    "    }\n",
    "    # model_class = RandomForestRegressor()\n",
    "    randomforest_reg_parameters = {\n",
    "        'n_estimators'     : [5,20,50,100], # number of trees in the random forest\n",
    "        'max_features'     : ['sqrt'], # number of features in consideration at every split\n",
    "        'min_samples_split': [2, 6, 10], # minimum sample number to split a node\n",
    "        'min_samples_leaf' : [1, 3, 4], # minimum sample number that can be stored in a leaf node\n",
    "        'bootstrap'        : [True, False] # method used to sample data points\n",
    "    }\n",
    "    # dictionary of model class and hyperparameters\n",
    "    regressor_models = {\n",
    "            'linear_regression' : [SGDRegressor(), sgd_reg_parameters],\n",
    "            'decision_tree'     : [DecisionTreeRegressor(), decisiontree_reg_parameters],\n",
    "            'gradient_boosting' : [GradientBoostingRegressor(), gradientboosting_reg_parameters],\n",
    "            'random_forests'    : [RandomForestRegressor(), randomforest_reg_parameters],\n",
    "        }\n",
    "\n",
    "    # model_class = LogisticRegression()\n",
    "    sgd_clf_parameters = {\n",
    "        'solver'         : ['newton-cg', 'sag', 'saga', 'lbfgs'],\n",
    "        'penalty'        : ['l2','none'],\n",
    "        'C'              : [1.0],\n",
    "        'max_iter'       : [100, 1000,2500, 5000]\n",
    "    }\n",
    "    # model_class = DecisionTreeClassifier()\n",
    "    decisiontree_clf_parameters = {\n",
    "        'criterion'               : [\"gini\",\"entropy\", \"log_loss\"],\n",
    "        'splitter'                : ['best', 'random'],\n",
    "        'max_depth'               : [1,3,5,7,12, None],\n",
    "        'min_samples_leaf'        : [1,3,4,5,7,8,10],\n",
    "        'min_weight_fraction_leaf': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "        'max_features'            : [\"log2\",\"sqrt\",None],\n",
    "        'max_leaf_nodes'          : [None,10,20,30,40,50,60,70,80,90],\n",
    "        'min_impurity_decrease'   : [0.0],\n",
    "        'ccp_alpha'               : [0.0]\n",
    "    }\n",
    "    # model_class = GradientBoostingClassifier()\n",
    "    gradientboosting_clf_parameters = {\n",
    "        'loss'              : ['log_loss', 'deviance', 'exponential'],\n",
    "        'learning_rate'     : [0.01,0.04],\n",
    "        'n_estimators'      : [100, 1500],\n",
    "        'subsample'         : [0.9, 0.1],\n",
    "        'min_samples_split' : [2],\n",
    "        'max_features'      : ['sqrt', 'log2'],\n",
    "        'min_samples_leaf'  : [1, 3, 4],\n",
    "        'max_depth'         : [4,10, None]\n",
    "    }\n",
    "    # model_class = RandomForestClassifier()\n",
    "    randomforest_clf_parameters = {\n",
    "        'n_estimators'             : [5,20,50,100], # number of trees in the random forest\n",
    "        'criterion'                : [\"gini\",\"entropy\", \"log_loss\"],\n",
    "        'max_depth'                : [1,3,5,7, None],\n",
    "        'max_features'             : ['log2', 'sqrt'], # number of features in consideration at every split\n",
    "        'min_samples_split'        : [2], # minimum sample number to split a node\n",
    "        'min_samples_leaf'         : [1, 3, 4], # minimum sample number that can be stored in a leaf node\n",
    "        'bootstrap'                : [True, False], # method used to sample data points\n",
    "        'min_weight_fraction_leaf' : [0.1,0.4,0.5,0.6,0.9],\n",
    "        'max_leaf_nodes'           : [None,10,20,60,70,80,90],\n",
    "        'min_impurity_decrease'    : [0.0],\n",
    "        'oob_score'                : [False],\n",
    "        'ccp_alpha'                : [0.0]\n",
    "    }\n",
    "    # dictionary of model class and hyperparameters\n",
    "    classifier_models = {\n",
    "        'logistic_regression' : [LogisticRegression(), sgd_clf_parameters],\n",
    "        'decision_tree'       : [DecisionTreeClassifier(), decisiontree_clf_parameters],\n",
    "        'gradient_boosting'   : [GradientBoostingClassifier(), gradientboosting_clf_parameters],\n",
    "        'random_forests'      : [RandomForestClassifier(), randomforest_clf_parameters],\n",
    "    }\n",
    "    \n",
    "    # select regression or classification based on task_folder\n",
    "    if task_folder == 'models/regression':\n",
    "        for key in regressor_models:\n",
    "            trained_model, model_score, best_model_param = tune_regression_model_hyperparameters(regressor_models[key][0],regressor_models[key][1], split_data, validation=2)\n",
    "            save_model(trained_model, best_model_param, model_score, folder=os.path.join(task_folder,key))\n",
    "    \n",
    "    elif task_folder == 'models/classification':\n",
    "        for key in classifier_models:\n",
    "            trained_model, model_score, best_model_param = tune_classification_model_hyperparameters(classifier_models[key][0],classifier_models[key][1], split_data, validation=2)\n",
    "            save_model(trained_model, best_model_param, model_score, folder=os.path.join(task_folder,key))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "379ccc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milestone-4, Task-7\n",
    "def find_best_model(task_folder: str) -> tuple[abc.ABCMeta, dict, dict]:\n",
    "    \"\"\"This function evaluates which model is best based on validation_rmse(for regression)/accuracy(for classification) \n",
    "    & returns:\n",
    "       -> the loaded model.\n",
    "       -> a dictionary of its hyperparameters.\n",
    "       -> a dictionary of its performance metrics.\n",
    "\n",
    "    Args:\n",
    "        task_folder (str): the name of the parent folder where all models metrics are compared to find the best model.\n",
    "    \n",
    "    Returns:\n",
    "        model (abc.ABCMeta): the model class.\n",
    "        performance_metrics (dict): return a dictionary of its performance metrics.\n",
    "        hyperparameters (dict): return a dictionary of its hyperparameter values.\n",
    "    \"\"\"\n",
    "    # an empty list to store the data frames\n",
    "    list_of_json_df = [] \n",
    "    path_of_the_directory = Path(task_folder)\n",
    "    # get data from json files in the 'path_of_the_directory' location\n",
    "    # store it in dataframe\n",
    "    # ignore .DS Store, *.joblib and 'neural_networks' folder\n",
    "    for filename in os.listdir(path_of_the_directory):\n",
    "        f = os.path.join(path_of_the_directory,filename)\n",
    "        if os.path.isfile(f):\n",
    "            pass\n",
    "        elif filename == 'neural_networks':\n",
    "            pass\n",
    "        else:\n",
    "            json_path = os.path.join(f,'metrics.json') \n",
    "            with open(json_path) as json_file:\n",
    "                # read data frame from json file\n",
    "                data = pd.read_json(json_file) \n",
    "                display(data)\n",
    "                # append the path to the data frame\n",
    "                data['Path'] = f\n",
    "                # append the data frame to the list\n",
    "                list_of_json_df.append(data)       \n",
    "    # concatenate all the data frames in the list.\n",
    "    df_metrics = pd.concat(list_of_json_df, ignore_index=True)\n",
    "    display(df_metrics)\n",
    "    # Select the path of the directory for the model with best metrics\n",
    "    # Regression\n",
    "    if task_folder == 'models/regression':\n",
    "        best_model_path = df_metrics[df_metrics.validation_RMSE==df_metrics.validation_RMSE.min()].Path.values[0]\n",
    "    # Classification\n",
    "    elif task_folder == 'models/classification':\n",
    "        # Get validation accuracy score for each model, which is at every other index+2 starting from 0\n",
    "        acc_result = df_metrics.loc[2::2, 'accuracy'].values\n",
    "        path_result = df_metrics.loc[2::2, 'Path'].values\n",
    "        # Find the index corresponding to max validation score\n",
    "        max_index = acc_result.argmax()\n",
    "        # Find the path corresponding to max validation score\n",
    "        best_model_path = path_result[max_index]\n",
    "    else:\n",
    "        pass\n",
    "    # Load the best model, a dictionary of its hyperparameters\n",
    "    # and a dictionary of its performance metrics.\n",
    "    for filename in os.listdir(best_model_path):\n",
    "        if filename=='metrics.json':\n",
    "            with open(os.path.join(best_model_path,'metrics.json')) as json_file:\n",
    "                performance_metrics = json.load(json_file)\n",
    "        elif filename=='hyperparameters.json':\n",
    "            with open(os.path.join(best_model_path,'hyperparameters.json')) as json_file1:\n",
    "                hyperparameters = json.load(json_file1)\n",
    "        elif filename=='model.joblib':\n",
    "            model = joblib.load(os.path.join(best_model_path,'model.joblib'))   \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return model, performance_metrics, hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bde2f4b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>Model_Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.378549</td>\n",
       "      <td>0.328342</td>\n",
       "      <td>0.300263</td>\n",
       "      <td>0.378549</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.325301</td>\n",
       "      <td>0.284357</td>\n",
       "      <td>0.267214</td>\n",
       "      <td>0.325301</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>0.389381</td>\n",
       "      <td>0.342507</td>\n",
       "      <td>0.316939</td>\n",
       "      <td>0.389381</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       accuracy  f1_score  precision    recall             Model_Class\n",
       "train  0.378549  0.328342   0.300263  0.378549  DecisionTreeClassifier\n",
       "test   0.325301  0.284357   0.267214  0.325301  DecisionTreeClassifier\n",
       "val    0.389381  0.342507   0.316939  0.389381  DecisionTreeClassifier"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>Model_Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.862776</td>\n",
       "      <td>0.863530</td>\n",
       "      <td>0.869683</td>\n",
       "      <td>0.862776</td>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.301205</td>\n",
       "      <td>0.291325</td>\n",
       "      <td>0.285923</td>\n",
       "      <td>0.301205</td>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>0.415929</td>\n",
       "      <td>0.417992</td>\n",
       "      <td>0.428831</td>\n",
       "      <td>0.415929</td>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       accuracy  f1_score  precision    recall                 Model_Class\n",
       "train  0.862776  0.863530   0.869683  0.862776  GradientBoostingClassifier\n",
       "test   0.301205  0.291325   0.285923  0.301205  GradientBoostingClassifier\n",
       "val    0.415929  0.417992   0.428831  0.415929  GradientBoostingClassifier"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>Model_Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.443218</td>\n",
       "      <td>0.435505</td>\n",
       "      <td>0.449619</td>\n",
       "      <td>0.443218</td>\n",
       "      <td>LogisticRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.313253</td>\n",
       "      <td>0.295390</td>\n",
       "      <td>0.295551</td>\n",
       "      <td>0.313253</td>\n",
       "      <td>LogisticRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>0.433628</td>\n",
       "      <td>0.401553</td>\n",
       "      <td>0.395336</td>\n",
       "      <td>0.433628</td>\n",
       "      <td>LogisticRegression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       accuracy  f1_score  precision    recall         Model_Class\n",
       "train  0.443218  0.435505   0.449619  0.443218  LogisticRegression\n",
       "test   0.313253  0.295390   0.295551  0.313253  LogisticRegression\n",
       "val    0.433628  0.401553   0.395336  0.433628  LogisticRegression"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>Model_Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.405363</td>\n",
       "      <td>0.363179</td>\n",
       "      <td>0.447978</td>\n",
       "      <td>0.405363</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.397590</td>\n",
       "      <td>0.362834</td>\n",
       "      <td>0.423337</td>\n",
       "      <td>0.397590</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>0.398230</td>\n",
       "      <td>0.334298</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.398230</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       accuracy  f1_score  precision    recall             Model_Class\n",
       "train  0.405363  0.363179   0.447978  0.405363  RandomForestClassifier\n",
       "test   0.397590  0.362834   0.423337  0.397590  RandomForestClassifier\n",
       "val    0.398230  0.334298   0.347685  0.398230  RandomForestClassifier"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>Model_Class</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.378549</td>\n",
       "      <td>0.328342</td>\n",
       "      <td>0.300263</td>\n",
       "      <td>0.378549</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>models\\classification\\decision_tree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.325301</td>\n",
       "      <td>0.284357</td>\n",
       "      <td>0.267214</td>\n",
       "      <td>0.325301</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>models\\classification\\decision_tree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.389381</td>\n",
       "      <td>0.342507</td>\n",
       "      <td>0.316939</td>\n",
       "      <td>0.389381</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>models\\classification\\decision_tree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.862776</td>\n",
       "      <td>0.863530</td>\n",
       "      <td>0.869683</td>\n",
       "      <td>0.862776</td>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>models\\classification\\gradient_boosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.301205</td>\n",
       "      <td>0.291325</td>\n",
       "      <td>0.285923</td>\n",
       "      <td>0.301205</td>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>models\\classification\\gradient_boosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.415929</td>\n",
       "      <td>0.417992</td>\n",
       "      <td>0.428831</td>\n",
       "      <td>0.415929</td>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>models\\classification\\gradient_boosting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.443218</td>\n",
       "      <td>0.435505</td>\n",
       "      <td>0.449619</td>\n",
       "      <td>0.443218</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>models\\classification\\logistic_regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.313253</td>\n",
       "      <td>0.295390</td>\n",
       "      <td>0.295551</td>\n",
       "      <td>0.313253</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>models\\classification\\logistic_regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.433628</td>\n",
       "      <td>0.401553</td>\n",
       "      <td>0.395336</td>\n",
       "      <td>0.433628</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>models\\classification\\logistic_regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.405363</td>\n",
       "      <td>0.363179</td>\n",
       "      <td>0.447978</td>\n",
       "      <td>0.405363</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>models\\classification\\random_forests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.397590</td>\n",
       "      <td>0.362834</td>\n",
       "      <td>0.423337</td>\n",
       "      <td>0.397590</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>models\\classification\\random_forests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.398230</td>\n",
       "      <td>0.334298</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.398230</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>models\\classification\\random_forests</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy  f1_score  precision    recall                 Model_Class  \\\n",
       "0   0.378549  0.328342   0.300263  0.378549      DecisionTreeClassifier   \n",
       "1   0.325301  0.284357   0.267214  0.325301      DecisionTreeClassifier   \n",
       "2   0.389381  0.342507   0.316939  0.389381      DecisionTreeClassifier   \n",
       "3   0.862776  0.863530   0.869683  0.862776  GradientBoostingClassifier   \n",
       "4   0.301205  0.291325   0.285923  0.301205  GradientBoostingClassifier   \n",
       "5   0.415929  0.417992   0.428831  0.415929  GradientBoostingClassifier   \n",
       "6   0.443218  0.435505   0.449619  0.443218          LogisticRegression   \n",
       "7   0.313253  0.295390   0.295551  0.313253          LogisticRegression   \n",
       "8   0.433628  0.401553   0.395336  0.433628          LogisticRegression   \n",
       "9   0.405363  0.363179   0.447978  0.405363      RandomForestClassifier   \n",
       "10  0.397590  0.362834   0.423337  0.397590      RandomForestClassifier   \n",
       "11  0.398230  0.334298   0.347685  0.398230      RandomForestClassifier   \n",
       "\n",
       "                                         Path  \n",
       "0         models\\classification\\decision_tree  \n",
       "1         models\\classification\\decision_tree  \n",
       "2         models\\classification\\decision_tree  \n",
       "3     models\\classification\\gradient_boosting  \n",
       "4     models\\classification\\gradient_boosting  \n",
       "5     models\\classification\\gradient_boosting  \n",
       "6   models\\classification\\logistic_regression  \n",
       "7   models\\classification\\logistic_regression  \n",
       "8   models\\classification\\logistic_regression  \n",
       "9        models\\classification\\random_forests  \n",
       "10       models\\classification\\random_forests  \n",
       "11       models\\classification\\random_forests  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(LogisticRegression(penalty='none', solver='saga'), {'accuracy': {'train': 0.443217665615142, 'test': 0.3132530120481928, 'val': 0.4336283185840708}, 'f1_score': {'train': 0.4355053446556361, 'test': 0.2953897286759771, 'val': 0.4015529686715437}, 'precision': {'train': 0.44961909792438326, 'test': 0.2955511043346255, 'val': 0.39533617236848356}, 'recall': {'train': 0.443217665615142, 'test': 0.3132530120481928, 'val': 0.4336283185840708}, 'Model_Class': 'LogisticRegression'}, {'C': 1.0, 'max_iter': 100, 'penalty': 'none', 'solver': 'saga'})\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"clean_tabular_data.csv\")\n",
    "#features,label = load_airbnb(dataset, label='Category', numeric=True)\n",
    "task_folder='models/regression'\n",
    "#split_data = get_split_data(features, label,task_folder='models/classification')\n",
    "#evaluate_all_models(task_folder, split_data)\n",
    "best_clf_model = find_best_model(task_folder)\n",
    "print(best_clf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83479b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif __name__ == \"__main__\":\\n\\n    dataset = pd.read_csv(\"clean_tabular_data.csv\")\\n\\n    # tune, train, save and find best regression model\\n    features,label = load_airbnb(dataset, label=\\'Price_Night\\', numeric=True)\\n    task_folder=\\'models/regression\\'\\n    split_data = get_split_data(features, label,task_folder=\\'models/regression\\')\\n    evaluate_all_models(task_folder, split_data)\\n    best_reg_model = find_best_model(task_folder)\\n    print(best_reg_model)\\n    \\n    # tune, train, save and find best classification model\\n    features,label = load_airbnb(dataset, label=\\'Category\\', numeric=True)\\n    task_folder=\\'models/classification\\'\\n    split_data = get_split_data(features, label,task_folder=\\'models/classification\\')\\n    evaluate_all_models(task_folder, split_data)\\n    best_clf_model = find_best_model(task_folder)\\n    print(best_clf_model)\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    dataset = pd.read_csv(\"clean_tabular_data.csv\")\n",
    "\n",
    "    # tune, train, save and find best regression model\n",
    "    features,label = load_airbnb(dataset, label='Price_Night', numeric=True)\n",
    "    task_folder='models/regression'\n",
    "    split_data = get_split_data(features, label,task_folder='models/regression')\n",
    "    evaluate_all_models(task_folder, split_data)\n",
    "    best_reg_model = find_best_model(task_folder)\n",
    "    print(best_reg_model)\n",
    "    \n",
    "    # tune, train, save and find best classification model\n",
    "    features,label = load_airbnb(dataset, label='Category', numeric=True)\n",
    "    task_folder='models/classification'\n",
    "    split_data = get_split_data(features, label,task_folder='models/classification')\n",
    "    evaluate_all_models(task_folder, split_data)\n",
    "    best_clf_model = find_best_model(task_folder)\n",
    "    print(best_clf_model)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Custom Tune Regression Model Hyperparameters\n",
    "#Comment out code above to run custom_tune_regression_model_hyperparameters()\n",
    "\n",
    "# Milestone-4, Task-3\n",
    "# define hyperparameter values\n",
    "model_class = SGDRegressor\n",
    "loss = ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
    "penalty = ['l1', 'l2', 'elasticnet', None]\n",
    "alpha = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "learning_rate = ['constant', 'optimal', 'invscaling', 'adaptive']\n",
    "epsilon = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "eta0 = [1, 10, 100]\n",
    "max_iter = [0, 0.5, 1, 10, 100, 1000, 5000, 10000]\n",
    "\n",
    "hyper_params = dict(loss=loss, penalty=penalty, alpha=alpha, learning_rate=learning_rate, epsilon=epsilon, eta0=eta0)\n",
    "\n",
    "dataset = pd.read_csv(\"clean_tabular_data.csv\")\n",
    "features,label = load_airbnb(dataset, label='Category', numeric=True)\n",
    "#Select task based on classification or regression\n",
    "task_folder='models/classification'\n",
    "#task_folder='models/regression'\n",
    "#split_data = get_split_data(features, label,task_folder)\n",
    "\n",
    "best_model, best_hyperparameter_values, model_score = custom_tune_regression_model_hyperparameters(model_class,split_data,hyper_params)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
